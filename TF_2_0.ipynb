{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF 2.0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fundou/tf/blob/main/TF_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvml-ji_7eNl"
      },
      "source": [
        "# A Primer on TensorFlow 2.0\n",
        "\n",
        "By [Akshay Agrawal](https://akshayagrawal.com). *This notebook accompanies [a post](https://www.debugmind.com/?p=2142) on my personal blog*.\n",
        "\n",
        "![TF 2.0 adopts a new look](https://www.debugmind.com/wp-content/uploads/2019/04/tf-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKrxS4J38vzV"
      },
      "source": [
        "From September 2017 to October 2018, I worked on TensorFlow 2.0 alongside many engineers. In this post, I’ll explain what TensorFlow 2.0 is and how it differs from TensorFlow 1.x. Towards the end, I’ll briefly compare TensorFlow 2.0 to PyTorch 1.0 and JAX. This post represents my own views; it does not represent the views of Google, my former employer.\n",
        "\n",
        "TensorFlow (TF) 2.0 is a significant, backwards-incompatible update to TF’s execution model and API.\n",
        "\n",
        "Execution model. In TF 2.0, all operations execute imperatively by default. Graphs and the graph runtime are both abstracted away by a just-in-time tracer that translates Python functions executing TF operations into executable graph functions. This means in TF 2.0, there is no `Session`, and no global graph state. The tracer is exposed as a Python decorator, tf.function. This decorator is for advanced users. Using it is completely optional.\n",
        "\n",
        "API. TF 2.0 makes tf.keras the high-level API for constructing and training neural networks. But you don’t have to use Keras if you don’t want to. You can instead use lower-level operations and automatic differentiation directly.\n",
        "\n",
        "To follow along with the code examples in this post, install the [TF 2.0 alpha ](https://tensorflow.org/alpha)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeNIxATdFaa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "0355d84c-ab0f-4b27-bf37-07ab997142a9"
      },
      "source": [
        "!pip install tensorflow==2.0.0-alpha0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.1.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (40.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.15.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb1fmdvPFw1S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7514d6b7-4dbf-4960-d047-a452b0444e34"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-alpha0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8vqclx39Wa-"
      },
      "source": [
        "## Why TF 2.0?\n",
        "\n",
        "TF 2.0 largely exists to make TF easier to use, for newcomers and researchers alike.\n",
        "\n",
        "\n",
        "### TF 1.x requires metaprogramming\n",
        "TF 1.x was designed to train extremely large, static neural networks. Representing a model as a dataflow graph and separating its specification from its execution simplifies training at scale, which explains why TF 1.x uses Python as a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) [metaprogramming](metaprogramming) tool for graphs.\n",
        "\n",
        "But most people don’t need to train Google-scale models, and most people find metaprogramming difficult. Constructing a TF 1.x graph is like writing assembly code, and this abstraction is so low-level that it is hard to produce anything but the simplest [differentiable programs](https://www.facebook.com/yann.lecun/posts/10155003011462143) using it. Programs that have data-dependent [control flow](https://en.wikipedia.org/wiki/Control_flow) are particularly hard to express as graphs.\n",
        "\n",
        "### Metaprogramming is (often) unnecessary\n",
        "It is possible to implement automatic differentiation by tracing computations while they are executed, without static graphs; [Chainer](https://chainer.org/), [PyTorch](https://openreview.net/pdf?id=BJJsrmfCZ), and [autograd](https://github.com/HIPS/autograd) do exactly that. These libraries are substantially easier to use than TF 1.x, since [imperative programming](https://en.wikipedia.org/wiki/Imperative_programming) is so much more natural than declarative programming. Moreover, when training models with large operations on a single machine, these graph-free libraries are competitive with TF 1.x performance. For these reasons, TF 2.0 privileges imperative execution.\n",
        "\n",
        "Graphs are still sometimes useful, for distribution, serialization, code generation, deployment, and (sometimes) performance. That’s why TF 2.0 provides the just-in-time tracer `tf.function`, which transparently converts Python functions into functions backed by graphs. This tracer also rewrites tensor-dependent Python control flow to TF control flow, and it automatically adds control dependencies to order reads and writes to TF state. This means that constructing graphs via `tf.function` is much easier than constructing TF 1.x graphs manually.\n",
        "\n",
        "### Multi-stage programming\n",
        "The ability to create polymorphic graph functions via `tf.function` at runtime makes TF 2.0 similar to a multi-stage programming language.\n",
        "\n",
        "For TF 2.0, I recommend the following multi-stage workflow. Start by implementing your program in imperative mode. Once you’re satisfied that your program is correct, measure its performance. If the performance is unsatisfactory, analyze your program using cProfile or a comparable tool to find bottlenecks consisting of TF operations. Next, refactor the bottlenecks into Python functions, and stage these functions in graphs with `tf.function`.\n",
        "\n",
        "![alt text](https://www.debugmind.com/wp-content/uploads/2019/04/msp.png)\n",
        "\n",
        "If you mostly use TF 2.0 to train large deep models, you probably won’t need to analyze or stage your programs. If on the other hand you write programs that execute lots of small operations, like MCMC samplers or reinforcement learning algorithms, you’ll likely find this workflow useful. In such cases, the Python overhead incurred by executing operations eagerly actually matters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkUBusNthPCX"
      },
      "source": [
        "## II. Imperative execution\n",
        "\n",
        "In TF 2.0, all operations are executed imperatively, or “eagerly”, by default. If you’ve used NumPy or PyTorch, TF 2.0 will feel familiar. For example, the following line of code will immediately construct two tensors backed by numerical tensors and then execute the add operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXUYGTHP7St3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6730198-f4dd-4c0a-9320-73c19d68d090"
      },
      "source": [
        "tf.constant([1., 2.]) + tf.constant([3., 4.])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=2, shape=(2,), dtype=float32, numpy=array([4., 6.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjmVL4FHhTf_"
      },
      "source": [
        "Contrast the above code snippet to its verbose, awkward TF 1.x equivalent:\n",
        "\n",
        "```\n",
        "# TF 1.X code\n",
        "x = tf.placeholder(tf.float32, shape=[2])\n",
        "y = tf.placeholder(tf.float32, shape=[2])\n",
        "value = x + y\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  print(sess.run(value, feed_dict={x: [1., 2.], y: [3., 4.]}))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2iV2K95hZec"
      },
      "source": [
        "In TF 2.0, there are no placeholders, no sessions, and no feed dicts. Because operations are executed immediately, you can use (and differentiate through) if statements and for loops (no more `tf.cond` or `tf.while_loop`). You can also use whatever Python data structures you like, and debug your programs with print statements and pdb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y-6zAFPhcXJ"
      },
      "source": [
        "If TF detects that a GPU is available, it will automatically run operations on the GPU when possible. The target device can also be controlled explicitly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85e29FeQc5so"
      },
      "source": [
        "if tf.test.is_gpu_available():\n",
        "  with tf.device('gpu:0'):\n",
        "    tf.constant([1., 2.]) + tf.constant([3., 4.])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIlkJAYUhfze"
      },
      "source": [
        "## III. State\n",
        "Using tf.Variable objects in TensorFlow required wrangling global collections of graph state, with confusing APIs like `tf.get_variable`, `tf.variable_scope`, and `tf.initializers.global_variables`. TF 2.0 does away with global collections and their associated APIs. If you need a `tf.Variable` in TF 2.0, then you just construct and initialize it directly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXbQUsTZy1ng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "38a19229-79c3-4d7c-a93d-03d014b98f38"
      },
      "source": [
        "tf.Variable(tf.random.normal([3, 5]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
              "array([[ 1.5562539 , -0.01612864, -0.6497036 ,  0.74280435,  1.0311264 ],\n",
              "       [ 0.06142396, -1.8662052 , -0.8031607 ,  2.1562974 ,  1.9559728 ],\n",
              "       [ 0.27328366,  1.087067  , -0.32779554, -0.08934236,  0.4248115 ]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MNjuOl2hzyU"
      },
      "source": [
        "## IV. Automatic differentiation\n",
        "\n",
        "TF 2.0 implements reverse-mode [automatic differentiation](https://books.google.com/books/about/Evaluating_Derivatives.html?id=xoiiLaRxcbEC) (also known as backpropagation), using a trace-based mechanism. This trace, or tape, is exposed as a context manager, `tf.GradientTape`. The watch method designates a `tf.Tensor` as something that we’ll need to differentiate with respect to later. Notice that by tracing the computation of `dy_dx` under the first tape, we’re able to compute `d2y_dx2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1y3gmZSzS4O"
      },
      "source": [
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape() as t1:\n",
        "  with tf.GradientTape() as t2:\n",
        "    t1.watch(x)\n",
        "    t2.watch(x)\n",
        "    y = x * x\n",
        "  dy_dx = t2.gradient(y, x)\n",
        "d2y_dx2 = t1.gradient(dy_dx, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unu38gAm0Rlv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ecf4692f-fdea-45be-c4c5-5b3b73b762ec"
      },
      "source": [
        "dy_dx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=24, shape=(), dtype=float32, numpy=6.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkgkvDzw0SKf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6f14289-9588-4fbe-ac7a-7fb0784435bb"
      },
      "source": [
        "d2y_dx2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=30, shape=(), dtype=float32, numpy=2.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMANtWmk0TCH"
      },
      "source": [
        "x = tf.Variable(3.0)\n",
        "with tf.GradientTape() as t1:\n",
        "  with tf.GradientTape() as t2:\n",
        "    y = x * x\n",
        "  dy_dx = t2.gradient(y, x)\n",
        "d2y_dx2 = t1.gradient(dy_dx, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NS7z05IiLxp"
      },
      "source": [
        "## V. Keras\n",
        "TF 1.x is notorious for having many mutually incompatible high-level APIs for neural networks. TF 2.0 has just one high-level API: `tf.keras`, which essentially implements the Keras API but is customized for TF. Several standard layers for neural networks are available in the tf.keras.layers namespace.\n",
        "\n",
        "Keras layers can be composed via `tf.keras.Sequential()` to obtain an object representing their composition. For example, the below code trains a toy CNN on MNIST. (Of course, MNIST can be solved by much simpler methods, like least squares.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c3i1pmrFbbW"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "input_shape=[28, 28, 1]\n",
        "data_format=\"channels_last\"\n",
        "\n",
        "max_pool = tf.keras.layers.MaxPooling2D(\n",
        "      (2, 2), (2, 2), padding='same', data_format=data_format)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Reshape(target_shape=input_shape, input_shape=[28, 28]),\n",
        "  tf.keras.layers.Conv2D(\n",
        "      32,\n",
        "      5,\n",
        "      padding='same',\n",
        "      data_format=data_format,\n",
        "      activation=tf.nn.relu),\n",
        "  max_pool,\n",
        "  tf.keras.layers.Conv2D(\n",
        "      64,\n",
        "      5,\n",
        "      padding='same',\n",
        "      data_format=data_format,\n",
        "      activation=tf.nn.relu),\n",
        "  max_pool,\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dropout(0.3),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbdqYFOMGr3W"
      },
      "source": [
        "model.compile(optimizer=tf.optimizers.Adam(),\n",
        "              loss=tf.losses.sparse_categorical_crossentropy,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TUB8qBwG-0M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f02e3229-d023-4b7b-d76d-ddb459a8d34d"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 213s 4ms/sample - loss: 0.3743 - accuracy: 0.9529\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb6cd99e128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MU9EsymiW_K"
      },
      "source": [
        "Alternatively, the same model could have been written as a subclass of `tf.keras.Model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSw8HFrQ3fow"
      },
      "source": [
        "class ConvNet(tf.keras.Model):\n",
        "  def __init__(self, input_shape, data_format):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.reshape = tf.keras.layers.Reshape(\n",
        "      target_shape=input_shape, input_shape=[28, 28])\n",
        "    self.conv1 = tf.keras.layers.Conv2D(32,5,\n",
        "      padding='same', data_format=data_format,\n",
        "      activation=tf.nn.relu)\n",
        "    self.pool = tf.keras.layers.MaxPooling2D(\n",
        "      (2, 2), (2, 2), padding='same', data_format=data_format)\n",
        "    self.conv2 = tf.keras.layers.Conv2D(64, 5,\n",
        "      padding='same', data_format=data_format,\n",
        "      activation=tf.nn.relu)\n",
        "    self.flt = tf.keras.layers.Flatten()\n",
        "    self.d1 = tf.keras.layers.Dense(1024, activation=tf.nn.relu)\n",
        "    self.dropout = tf.keras.layers.Dropout(0.3)\n",
        "    self.d2 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.reshape(x)\n",
        "    x = self.conv1(x)\n",
        "    x = self.pool(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.pool(x)\n",
        "    x = self.flt(x)\n",
        "    x = self.d1(x)\n",
        "    x = self.dropout(x)\n",
        "    return self.d2(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zCFDWVbia3N"
      },
      "source": [
        "If you don’t want to use `tf.keras`, you can use low-level APIs like `tf.reshape`, `tf.nn.conv2d`, `tf.nn.max_pool`, `tf.nn.dropout`, and `tf.matmul` directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A019eVCdijB5"
      },
      "source": [
        "## VI. Graph functions\n",
        "For advanced users who need graphs, TF 2.0 provides `tf.function`, a just-in-time tracer that converts Python functions that execute TensorFlow operations into graph functions. A graph function is a TF graph with named inputs and outputs. Graph functions are executed by a C++ runtime that automatically partitions graphs across devices, and it parallelizes and optimizes them before execution.\n",
        "\n",
        "Calling a graph function is syntactically equivalent to calling a Python function. Here’s a very simple example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bz-J807_ClB"
      },
      "source": [
        "@tf.function\n",
        "def add(tensor):\n",
        "  return tensor + tensor + tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XYoTmRf_Fl-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b1078156-491d-4b19-9cc4-b9e428c6b659"
      },
      "source": [
        "add(tf.ones([2, 2])) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=16125, shape=(2, 2), dtype=float32, numpy=\n",
              "array([[3., 3.],\n",
              "       [3., 3.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHt8ZEMKio4r"
      },
      "source": [
        "The `add` function is also polymorphic in the data types and shapes of its Tensor arguments (and the run-time values of the non-Tensor arguments), even though TF graphs are not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9bLplRh_Ipm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b108a144-3a58-4736-c768-a726121b95aa"
      },
      "source": [
        "add(tf.ones([2, 2], dtype=tf.uint8)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=16137, shape=(2, 2), dtype=uint8, numpy=\n",
              "array([[3, 3],\n",
              "       [3, 3]], dtype=uint8)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7UI55Skis-H"
      },
      "source": [
        "Every time a graph function is called, its “input signature” is analyzed. If the input signature doesn’t match an input signature it has seen before, it re-traces the Python function and constructs another concrete graph function. (In programming languages terms, this is like [multiple dispatch](https://en.wikipedia.org/wiki/Multiple_dispatch) or [lightweight modular staging](https://infoscience.epfl.ch/record/150347/files/gpce63-rompf.pdf).) This means that for one Python function, many concrete graph functions might be constructed. This also means that every call that triggers a trace will be slow, but subsequent calls with the same input signature will be much faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb_FhLJsi0cC"
      },
      "source": [
        "### Lexical closure, state, and control dependencies\n",
        "Graph functions support lexically closing over `tf.Tensor` and `tf.Variable` objects. You can mutate `tf.Variable` objects inside a graph function, and `tf.function` will automatically add the control dependencies needed to ensure that your reads and writes happen in program-order.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIW_NK1w_KyO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60a0d6d6-1f93-484c-a3ca-e852305ecbdc"
      },
      "source": [
        "a = tf.Variable(1.0)\n",
        "b = tf.Variable(1.0)\n",
        "\n",
        "@tf.function\n",
        "def f(x, y):\n",
        "  a.assign(y * b)\n",
        "  b.assign_add(x * a)\n",
        "  return a + b\n",
        "\n",
        "f(tf.constant(1.0), tf.constant(2.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=16174, shape=(), dtype=float32, numpy=5.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTa93ObIHbvh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "611a3ad4-d3d6-4743-f02d-fe8200fa4c68"
      },
      "source": [
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcw2JHggHeHp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a2272df-f01f-4dfd-efa0-365fdef46c1a"
      },
      "source": [
        "b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IplYW2Vui8h3"
      },
      "source": [
        "### Python control flow\n",
        "`tf.function` automatically rewrites Python control flow that depends on `tf.Tensor` data into graph control flow, using autograph. This means that you no longer need to use constructs like `tf.cond` and `tf.while_loop`. For example, if we were to translate the following function into a graph function via `tf.function`,  autograph would convert the for loop into a `tf.while_loop`, because it depends on `tf.range(100)`, which is a `tf.Tensor`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkLCQ2NiLJyN"
      },
      "source": [
        "# Decorating this with 'tf.function' would rewrite the for loop using tf.while\n",
        "def matmul_many(tensor):\n",
        "  accum = tensor\n",
        "  for _ in tf.range(100):\n",
        "    accum = tf.matmul(accum, tensor)\n",
        "  return accum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abiRV_q7jGJA"
      },
      "source": [
        "It’s important to note that if `tf.range(100)` were replaced with `range(100)`, then the loop would be unrolled, meaning that a graph with 100 matmul operations would be generated.\n",
        "\n",
        "You can inspect the code that autograph generates on your behalf."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxIrW4CrLLSy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "da2983e2-3e21-4b86-ef82-1cb4ba9f73aa"
      },
      "source": [
        "print(tf.autograph.to_code(matmul_many))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from __future__ import print_function\n",
            "\n",
            "def tf__matmul_many(tensor):\n",
            "  try:\n",
            "    with ag__.function_scope('matmul_many'):\n",
            "      do_return = False\n",
            "      retval_ = None\n",
            "      accum = tensor\n",
            "\n",
            "      def loop_body(loop_vars, accum_1):\n",
            "        with ag__.function_scope('loop_body'):\n",
            "          _ = loop_vars\n",
            "          accum_1 = ag__.converted_call('matmul', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=ag__.Feature.ALL, internal_convert_user_code=True), (accum_1, tensor), {})\n",
            "          return accum_1,\n",
            "      accum, = ag__.for_stmt(ag__.converted_call('range', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=ag__.Feature.ALL, internal_convert_user_code=True), (100,), {}), None, loop_body, (accum,))\n",
            "      do_return = True\n",
            "      retval_ = accum\n",
            "      return retval_\n",
            "  except:\n",
            "    ag__.rewrite_graph_construction_error(ag_source_map__)\n",
            "\n",
            "\n",
            "\n",
            "tf__matmul_many.autograph_info__ = {}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgB9CWjsjkeg"
      },
      "source": [
        "### Performance\n",
        "\n",
        "Graph functions can provide significant speed-ups for programs that execute many small TF operations. For these programs, the Python overhead incurred executing an operation imperatively outstrips the time spent running the operations. As an example, let’s benchmark the `matmul_many` function imperatively and as a graph function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP459YhLBpIF"
      },
      "source": [
        "graph_fn = tf.function(matmul_many)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPZqBZCBjph7"
      },
      "source": [
        "Here’s the imperative (Python) performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC73w7oMBCq_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9861acd5-040c-4378-e0a2-d612ccdb6aef"
      },
      "source": [
        "%%timeit\n",
        "\n",
        "matmul_many(tf.ones([2, 2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 loops, best of 3: 12.1 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9cayEkbjsnm"
      },
      "source": [
        "The first call to `graph_fn` is slow, since this is when the graph function is generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE3_sabyCJG1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "70835c99-2c28-4057-d8b3-5fbaf9e34eb6"
      },
      "source": [
        "%%time\n",
        "\n",
        "graph_fn(tf.ones([2, 2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 147 ms, sys: 2.95 ms, total: 150 ms\n",
            "Wall time: 150 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=224621, shape=(2, 2), dtype=float32, numpy=\n",
              "array([[1.2676506e+30, 1.2676506e+30],\n",
              "       [1.2676506e+30, 1.2676506e+30]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mkM4GM6jyhh"
      },
      "source": [
        "But subsequent calls are an order of magnitude faster than imperatively executing `matmul_many`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQC91XrtCNpm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "969d2da0-fbf3-4e93-8d03-1b09952fe97c"
      },
      "source": [
        "%%timeit\n",
        "\n",
        "graph_fn(tf.ones([2, 2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000 loops, best of 3: 1.81 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGmI6VYqj-mg"
      },
      "source": [
        "## VII. Comparison to other Python libraries\n",
        "\n",
        "There are many libraries for machine learning. Out of all of them, PyTorch 1.0 is the one that’s most similar to TF 2.0. Both TF 2.0 and PyTorch 1.0 execute imperatively by default, and both provide ways to transform Python functions into graph-backed functions (compare `tf.function` and [torch.jit](https://pytorch.org/docs/stable/jit.html#)). The PyTorch JIT tracer, `torch.jit.trace`, doesn’t implement the multiple-dispatch semantics that `tf.function` does, and it also doesn’t rewrite the AST. On the other hand, `TorchScript` lets you use Python control flow, but unlike `tf.function`, it doesn’t let you mix in arbitrary Python code that parametrizes the construction of your graph. That means that in comparison to `tf.function`, `TorchScript` makes it harder for you to shoot yourself in the foot, while potentially limiting your creative expression.\n",
        "\n",
        "So should you use TF 2.0, or PyTorch 1.0? It depends. Because TF 2.0 is in alpha, it still has some kinks, and its imperative performance still needs work. But you can probably count on TF 2.0 becoming stable sometime this year. If you’re in industry, TensorFlow has [TFX](https://www.tensorflow.org/tfx/) for production pipelines and [TFLite](https://www.tensorflow.org/lite) for deploying to mobile. PyTorch recently made a [commitment to production](https://pytorch.org/blog/the-road-to-1_0/); since then, they’ve added [C++ inference](https://pytorch.org/tutorials/advanced/cpp_export.html) and deployment solutions for several cloud providers. For research, I’ve found that TF 2.0 and PyTorch 1.0 are sufficiently similar that I’m comfortable using either one, and my choice of framework depends on my collaborators.\n",
        "\n",
        "The multi-stage approach of TF 2.0 is similar to what’s done in [JAX](https://github.com/google/jax/). JAX is great if you want a functional programming model that looks exactly like NumPy, but with automatic differentiation and GPU support; this is, in fact, what many researchers want. If you don’t like functional programming, JAX won’t be a good fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MajIFXj8kf7o"
      },
      "source": [
        "## VIII. Domain-specific languages for machine learning\n",
        "TF 2.0 and PyTorch 1.0 are very unusual libraries. [It has been observed](https://julialang.org/blog/2017/12/ml&pl) that these libraries resemble domain-specific languages (DSLs) for automatic-differentiation and machine learning, embedded in Python (see also [our paper on TF Eager](https://arxiv.org/pdf/1903.01855.pdf), TF 2.0’s precursor). What TF 2.0 and PyTorch 1.0 accomplish in Python is impressive, but they’re pushing the language to its limits.\n",
        "\n",
        "There is now significant work underway to embed ML DSLs in languages that are more amenable to compilation than Python, like Swift ([DLVM](https://arxiv.org/pdf/1711.03016.pdf), [Swift for TensorFlow](https://github.com/tensorflow/swift), [MLIR](https://drive.google.com/file/d/1hUeAJXcAXwz82RXA5VtO5ZoH8cVQhrOK/view)), and Julia ([Flux](https://github.com/FluxML/Flux.jl), [Zygote](https://github.com/FluxML/Zygote.jl)). So while TF 2.0 and PyTorch 1.0 are great libraries, do stay tuned: over the next year (or two, or three?), the ecosystem of programming languages for machine learning will continue to evolve rapidly."
      ]
    }
  ]
}